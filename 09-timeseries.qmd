# Time series data {#sec-noneuclidean}

```{r}
#| include: false
#| cache: false
source("before-each-chapter.R")
tscv_plot <- function(.init, .step, h = 1) {
  expand.grid(
    time = seq(26),
    .id = seq(trunc(11 / .step))
  ) |>
    group_by(.id) |>
    mutate(
      observation = case_when(
        time <= ((min(.id) - 1) * .step + .init) ~ "train",
        time %in% c((min(.id) - 1) * .step + .init + h) ~ "test",
        TRUE ~ "unused"
      )
    ) |>
    ungroup() |>
    filter(.id <= 26 - .init) |>
    ggplot(aes(x = time, y = .id)) +
    geom_segment(
      aes(x = 0, xend = 27, y = .id, yend = .id),
      arrow = arrow(length = unit(0.015, "npc")),
      col = "black", linewidth = .25
    ) +
    geom_point(aes(col = observation), size = 2) +
    scale_y_reverse() +
    scale_color_manual(values = c(train = "#0072B2", test = "#c14b14", unused = "gray")) +
    guides(col = "none") +
    labs(x = "time", y = "") +
    theme_void() +
    theme(axis.title.x = element_text(margin = margin(t = 2))) +
    theme(text = ggplot2::element_text(family = "Fira Sans"))
}

pbs_plot <- function(pbs, anomalies, series) {
  dates <- anomalies |>
    dplyr::filter(ATC2 == series) |>
    dplyr::pull(Month) |>
    as.Date()
  pbs |>
    dplyr::filter(ATC2 == series) |>
    ggplot2::ggplot(ggplot2::aes(x = Month, y = Scripts)) +
    tsibble::scale_x_yearmonth(date_breaks = "2 years", date_labels = "%Y") +
    ggplot2::geom_vline(xintercept = dates, color = "red", alpha = 0.4) +
    ggplot2::geom_line() +
    ggplot2::labs(title = paste("Scripts for ATC group", series))
}
```

For this chapter, we will load some additional packages designed to work with time series data. To learn more about these packages and their use in time series analysis, see @fpp3.

```{r}
#| label: tsibble
#| code-fold: false
#| warning: false
#| message: false
library(tsibble)
library(fable)
library(feasts)
library(broom)
library(tsibbledata)
```

## Time series anomaly detection paradigms

Time series data are observations that are collected over time. In this book, we will only consider time series that are observed at regular intervals, such as annually, monthly, or hourly.

When considering time series data, we can distinguish between three different anomaly detection paradigms:

1. **Weird times**: Identifying anomalies within a time series in historical data.
1. **Weird series**: Identifying an anomalous time series within a collection of time series.
1. **Surveillance**: Identifying anomalies within a time series in real time

These can be illustrated using the following examples.

```{r}
#| label: fig-tsparadigms
#| echo: false
#| fig-cap: The three main anomaly detection paradigms. In the top panel, we aim to identify unusual observations within historical data. In the middle panel, we aim to identify an unusual time series within a collection of time series. In the bottom panel, we aim to identify an unusual observation in the next time period.
a12 <- tsibbledata::PBS |>
  group_by(ATC2) |>
  summarise(Scripts = sum(Scripts) / 1e3) |>
  ungroup() |>
  filter(ATC2 == "A12", Month <= yearmonth("2006 Feb")) |>
  mutate(paradigm = "Surveillance")
p1 <- fr_mortality |>
  filter(Age == 25, Sex == "Male") |>
  mutate(paradigm = "Weird times") |>
  ggplot(aes(x = Year, y = Mortality)) +
  geom_line() +
  facet_grid(paradigm ~ .) +
  theme(axis.title = element_blank()) +
  scale_x_yearmonth(breaks = NULL)
p2 <- fr_mortality |>
  filter(Age %in% c(2:6, 60), Sex == "Female") |>
  mutate(paradigm = "Weird series") |>
  ggplot(aes(x = Year, y = Mortality, color = as.factor(Age))) +
  geom_line() +
  guides(col = "none") +
  facet_grid(paradigm ~ .) +
  theme(axis.title = element_blank()) +
  scale_x_yearmonth(breaks = NULL) +
  scale_y_log10(breaks = NULL)
p3 <- a12 |>
  autoplot(Scripts) +
  geom_point(data = tail(a12, 1)) +
  facet_grid(paradigm ~ .) +
  theme(axis.title = element_blank()) +
  scale_x_yearmonth(breaks = NULL) +
  scale_y_continuous(breaks = NULL)

patchwork::wrap_plots(p1, p2, p3, ncol = 1)
```

**Weird times**: In the top plot, we are looking for historical anomalies within time series data. This is common in quality control, or in data cleaning. The aim is to find time periods where the observations are different from the rest of the data. Again, it can easily be extended to multivariate time series, where we look for time periods when one or more of the series may display unusual observations.

**Weird series**: In the middle plot, we are looking for an anomalous time series within a collection of time series. The observations within each series may all be consistent, but the series as a whole may be unusual. This is, by its nature, a multivariate problem. It is common in finance, where we look for unusual behaviour of a stock within a collection of stocks, or in health, where we look for unusual behaviour of a patient within a collection of patients.

**Surveillance**: In the bottom plot, we are looking for an anomaly in the next observation in the time series. This is commonly done in surveillance, when a time series is monitored in real time to identify an unusual observation, and appropriate action taken. An anomaly in this context, is an observation that is very different from what was forecast. Although only a single time series is shown in this plot, the idea easily extends to multivariate time series, where we look for something unusual occurring in the next time period across a set of time series.

We will discuss each of these paradigms in turn.

## Weird times

As a vehicle of illustration, we will consider French mortality rates, disaggregated by age and sex. These are obtained from the @HMD. We will consider male and female data from `r min(fr_mortality$Year)` to `r max(fr_mortality$Year)` over ages `r min(fr_mortality$Age)` to `r max(fr_mortality$Age)`, giving `r fr_mortality |> select(Age, Sex) |> distinct() |> NROW()` separate time series, each of length `r max(fr_mortality$Year) - min(fr_mortality$Year) + 1`. In fact, the top plot in @fig-tsparadigms shows the male mortality rates for 25 year olds over this time period.

```{r}
#| label: fr_mortality
#| code-fold: false
fr_mortality
```

@fig-fr_mortality_time_plots shows the data for both sexes and all ages. The mortality rates have improved over time for all ages, especially since 1950. Infant mortality (age 0) is much higher than the rest of childhood, with mortality rates at a minimum at about age 10 in all years. The highest mortality rates are for the oldest age groups. The effect of the two wars are particularly evident in the male mortality rates.

```{r}
#| label: fig-fr_mortality_time_plots
#| code-fold: false
#| fig-cap: !expr sprintf("French mortality rates by sex and age from %d to %d.  We use a log scale because the rates are vastly different for different age groups.", min(fr_mortality$Year), max(fr_mortality$Year))
fr_mortality |>
  ggplot(aes(x = Year, y = Mortality, color = Age, group = Age)) +
  geom_line() +
  facet_grid(. ~ Sex) +
  scale_y_log10(labels = scales::comma)
```

Anomaly detection methods for identifying weird time anomalies are usually based on fitting a smooth function $m_t$ through the observations, estimating the spread in the series $s_t$, and computing the standardized residuals $e_t = (y_t - m_t)/s_t$. We then identify anomalies in the $e_t$ series using one of the methods discussed in @sec-density-methods or @sec-distance-methods. To illustrate, we will consider two such approaches, but many more are possible depending on how $m_t$ and $s_t$ are estimated, and how anomalies are identified in $e_t$.

### Hampel identifier

The Hampel identifier is due to a proposal of Frank Hampel [@Davies1993], and is designed to identify anomalies in a time series based on whether an observation is very different from the neighbouring observations. Suppose we define neighbourhoods comprising observations within $h$ time periods. Then the local median is given by
$$
\hat{m}_t = \text{median}(y_{t-h}, \dots, y_{t+h}).
$$
This is also called a "moving median", a "running median" or a "rolling median". Similarly, the local median absolute deviation (MAD) is given by
$$
\hat{s}_t = \text{median}(|y_{t-h}-\hat{m}_t|, \dots, |y_{t+h}- \hat{m}_t|).
$$
This measures the variation of the series in the neighbourhood of time $t$. The residuals $e_t = (y_t-\hat{m}_t)/s_t$ provide a measure for how different an observation $y_t$ is from its neighbours. The Hampel identifier denotes an observation as an anomaly if
$$|e_t| > \tau$$
for some threshold $\tau$. A related idea is the Hampel filter, where each anomaly is replaced by the local median $\hat{m}_t$.

The threshold is often set under the assumption of a Normal distribution, for which the MAD is equal to 0.67445 times the standard deviation. So it is common to set $\tau = k/0.67445$ where $k$ is the number of standard deviations permitted before an observation is considered an outlier. Because we apply the rule repeatedly over the length of a time series, we need to be careful of the problem of multiple comparisons, where the probability of a false positive overall is much larger than the probability of a false positive for each individual test. However, it is not straightforward to adjust the probabilities because the tests are not independent, and the dependency between them depends on the window size $h$, and the characteristics of the time series. Setting $k=5$ or 6 seems to work quite well for many problems.

The `hampel_anomalies()` function implements this algorithm. The code below applies the algorithm to all 172 time series in `fr_mortality`.

```{r}
#| label: hampel-anomalies
#| code-fold: false
fr_anomalies <- fr_mortality |>
  group_by(Age, Sex) |>
  mutate(hampel = hampel_anomalies(Mortality, bandwidth = 7, k = 6)) |>
  ungroup() |>
  filter(hampel) |>
  arrange(Year, Age)
fr_anomalies
```

```{r}
#| label: fig-hampel_fr
#| code-fold: true
#| warning: false
#| fig-cap: French mortality observations identified as anomalies using the Hampel identifier.
#| dependson: hampel_anomalies
# Find years containing anomalies
yrs <- fr_anomalies |>
  select(Year, Sex) |>
  distinct()
fr_anomalies_plot_male2 <- fr_anomalies |>
  filter(Sex == "Male") |>
  ggplot(aes(x = Year, y = Age)) +
  facet_grid(. ~ Sex) +
  scale_x_continuous(
    breaks = seq(1820, 2000, by = 20),
    limits = range(yrs$Year)
  ) +
  geom_vline(
    xintercept = unique(yrs$Year[yrs$Sex == "Male"]),
    alpha = 0.5, color = "grey"
  ) +
  geom_point(col = "#478cb2") +
  ggrepel::geom_text_repel(
    data = yrs |>
      filter(Sex == "Male", !Year %in% 1915:1918),
    aes(y = 75, label = Year), col = "#478cb2", size = 3, seed = 1967
  ) +
  ylim(4, 85)
fr_anomalies_plot_female2 <- fr_anomalies |>
  filter(Sex == "Female") |>
  ggplot(aes(x = Year, y = Age)) +
  facet_grid(. ~ Sex) +
  scale_x_continuous(
    breaks = seq(1820, 2000, by = 20),
    limits = range(yrs$Year)
  ) +
  geom_vline(
    xintercept = unique(yrs$Year[yrs$Sex == "Female"]),
    alpha = 0.5, color = "grey"
  ) +
  labs(title = "French mortality anomalies") +
  geom_point(col = "#c1653a") +
  ggrepel::geom_text_repel(
    data = yrs[yrs$Sex == "Female", ],
    aes(y = 75, label = Year), col = "#c1653a", size = 3, seed = 1967
  ) +
  ylim(4, 85)
patchwork::wrap_plots(
  fr_anomalies_plot_female2,
  fr_anomalies_plot_male2,
  nrow = 1
)
```

@fig-hampel_fr shows the `r NROW(fr_anomalies)` observations that have been identified as anomalies, showing the wars and epidemics that have occurred over time. The following events in French history are evident in the data:

* 1832, 1849, 1854: Cholera outbreaks
* 1853-1856: Crimean war
* 1870: Franco-Prussian war
* 1871: Repression of the ‘Commune de Paris’
* 1914-1918: World War I
* 1918: Spanish flu outbreak
* 1940-1944: World War II

### Surprisal anomalies

The Hampel identifier worked well for the annual mortality series, but the local median approach will break down when there is seasonality or other systematic patterns in the data. However, the same idea can be adapted by replacing the local median with a more sophisticated way of modelling the signal in the data. One such method is STL (Seasonal Trend decomposition using Loess) due to @Cleveland1990, and extended by @mstl to handle multiple seasonal periods or series with no seasonality. In the latter case, Friedman's SuperSmoother [@supsmu] is used to estimate the signal $m_t$. This uses local linear regressions rather than local medians, with the window width chosen by cross-validation. Other estimates of $m_t$ are also possible, provided they are robust to outliers.

```{r}
#| label: fr_fit
#| code-fold: false
fr_fit <- fr_mortality |>
  as_tsibble(index = Year, key = c(Age, Sex)) |>
  model(stl = STL(Mortality)) |>
  augment() |>
  as_tibble() |>
  select(-.model, -.innov)
fr_fit
```

The `.fitted` column contains the estimates of $m_t$, while the `.resid` column contains the differences $y_t - \hat{m}_t$. We will estimate $s_t$ using a robust estimate of standard deviation across each series (rather than locally, as was done with the Hampel method). This time, we will use IQR, rather than MAD, to estimate the spread, with a similar adjustment so that it is equivalent to a standard deviation if the data were normally distributed.

```{r}
#| label: fr_scores1
#| code-fold: false
#| dependson: fr_fit
fr_sd <- fr_fit |>
  group_by(Age, Sex) |>
  summarise(s = IQR(.resid) / 1.349, .groups = "drop")
fr_scores <- fr_fit |>
  left_join(fr_sd, by = c("Age", "Sex")) |>
  mutate(e = .resid / s)
fr_scores
```

At this point, we could identify a point as an anomaly if it is more than $k$ standard deviations away from $m_t$, as was done with the Hampel identifier. This is also the approach taken by the `tsoutliers` function from the `forecast` package, which uses 3 IQR as the threshold, equivalent to $k=4.05$ standard deviations. But we will use density scores instead, and compute the corresponding surprisal probabilities assuming the residuals follow a Normal distribution.

```{r}
#| label: fr_scores2
#| code-fold: false
#| dependson: fr_scores1
fr_scores <- fr_scores |>
  mutate(
    s = -dnorm(e, log = TRUE),
    prob = surprisals(s)
  )
fr_scores |> arrange(prob)
```

The most extreme anomalies all correspond to male deaths due to World War I. A plot similar to @fig-hampel_fr can be made, showing points with lookout probability less than 0.05. Fewer anomalies have been identified than with the Hampel identifier. Because the lookout probabilities were based on a Generalized Pareto Distribution with a threshold of 90%, and we are identifying anomalies with probability less than 0.05, the overall probability of a false positive is 0.005 per series, so it is unlikely that we have identified any false positives, and likely that we have missed some anomalies.

```{r}
#| label: fig-stl_anomalies
#| code-fold: true
#| warning: false
#| fig-cap: French mortality observations identified as anomalies using density scores obtained from applying Friedman's supersmoother to each series.
#| dependson: fr_scores2
fr_anomalies <- fr_scores |>
  filter(prob < 0.05) |>
  as_tibble() |>
  select(Year, Sex, Age) |>
  distinct() |>
  left_join(fr_mortality, by = c("Age", "Sex", "Year"))
yrs <- fr_anomalies |>
  select(Year, Sex) |>
  distinct()
fr_anomalies_plot_male2 <- fr_anomalies |>
  filter(Sex == "Male") |>
  ggplot(aes(x = Year, y = Age)) +
  facet_grid(. ~ Sex) +
  scale_x_continuous(
    breaks = seq(1820, 2000, by = 20),
    limits = range(yrs$Year)
  ) +
  geom_vline(
    xintercept = unique(yrs$Year[yrs$Sex == "Male"]),
    alpha = 0.5, color = "grey"
  ) +
  geom_point(col = "#478cb2") +
  ggrepel::geom_text_repel(
    data = yrs[yrs$Sex == "Male", ],
    aes(y = 75, label = Year), col = "#478cb2", size = 3, seed = 1967
  ) +
  ylim(4, 85)
fr_anomalies_plot_female2 <- fr_anomalies |>
  filter(Sex == "Female") |>
  ggplot(aes(x = Year, y = Age)) +
  facet_grid(. ~ Sex) +
  scale_x_continuous(
    breaks = seq(1820, 2000, by = 20),
    limits = range(yrs$Year)
  ) +
  geom_vline(
    xintercept = unique(yrs$Year[yrs$Sex == "Female"]),
    alpha = 0.5, color = "grey"
  ) +
  labs(title = "French mortality anomalies") +
  geom_point(col = "#c1653a") +
  ggrepel::geom_text_repel(
    data = yrs[yrs$Sex == "Female", ],
    aes(y = 75, label = Year), col = "#c1653a", size = 3, seed = 1967
  ) +
  ylim(4, 85)
patchwork::wrap_plots(
  fr_anomalies_plot_female2,
  fr_anomalies_plot_male2,
  nrow = 1
)
```

### Multivariate time series

A similar can be applied to the whole collection of time series, where we are looking for years that are anomalous across all ages and both sexes. One way to do this is to first take principal components of the series, and then apply the preceding procedure to the series of first principal component scores.

First we convert the series to logs as they are on vastly different scales, then we transform the data to wide form. Finally, we compute the first principal component of the resulting matrix.

```{r}
# Convert to wide form
fr_wide <- fr_mortality |>
  mutate(logm = log(Mortality)) |>
  mutate(series = paste0(substr(Sex, 1, 1), ":", Age)) |>
  select(-Age, -Sex, -Mortality) |>
  tidyr::pivot_wider(names_from = series, values_from = logm)
# Compute 1 principal component from mortality rates
pcs <- fr_wide |>
  select(-Year) |>
  prcomp(rank = 1) |>
  broom::augment(fr_wide |> select(Year))
# Plot principal component scores
pcs |>
  ggplot(aes(x = Year, y = .fittedPC1)) +
  geom_line()
```

Now we can apply an STL model to find anomalies in this series.

```{r}
pcs |>
  as_tsibble(index = Year) |>
  model(stl = STL(.fittedPC1)) |>
  augment() |>
  select(-.model, -.innov) |>
  mutate(e = .resid / IQR(.resid) * 1.349) |>
  filter(abs(e) > 3)
```

## Weird series

Next we consider unusual series within a large collection of time series.

To illustrate the ideas we will use a data set of monthly observations on
the Australian Pharmaceutical Benefits Scheme (PBS), from July 1991 to June 2008. The PBS involves the Australian government subsidising certain pharmaceutical products, to allow more equitable access to essential medicines. The data set contains monthly sales volumes of those products being subsidised, classified according to the Anatomical Therapeutic Chemical (ATC) classification system.


```{r}
#| label: weirdseries
#| message: false
#| warning: false
#| code-fold: false
## Compute features (and omit series with missing features)
PBS_feat <- PBS |>
  features(Cost, feature_set(pkgs = "feasts")) |>
  select(-`...26`) |>
  na.omit()

## Compute principal components
PBS_prcomp <- PBS_feat |>
  select(-Concession, -Type, -ATC1, -ATC2) |>
  prcomp(scale = TRUE) |>
  augment(PBS_feat)

## Plot the first two components
PBS_prcomp |>
  ggplot(aes(x = .fittedPC1, y = .fittedPC2)) +
  geom_point()

## Pull out most unusual series from first principal component
outliers <- PBS_prcomp |>
  filter(.fittedPC1 > 6)
outliers |>
  select(ATC1, ATC2, Type, Concession)

## Visualise the unusual series
PBS |>
  semi_join(outliers, by = c("Concession", "Type", "ATC1", "ATC2")) |>
  autoplot(Cost) +
  facet_grid(vars(Concession, Type, ATC1, ATC2)) +
  labs(title = "Outlying time series in PC space")
```

These series are either all zeros, or have a single non-zero observation.


## Surveillance

In surveillance, we are interested in identifying anomalies in real time. This is common in monitoring systems, where we want to identify unusual behaviour as soon as it occurs. For example, in a manufacturing plant, we may want to identify when a machine is not operating as expected, and take action to prevent further problems. In retail, we may want to identify when sales are unusually high or low, and adjust stock levels if necessary.

Again, we will use the PBS data. For this example, we will combine the data into ATC level 2 groups, and look at total sales volumes (measured in thousands of scripts).

```{r}
#| label: pbs
pbs <- PBS |>
  arrange(ATC2, Month) |>
  group_by(ATC2) |>
  summarise(Scripts = sum(Scripts) / 1e3, .groups = "drop") |>
  mutate(t = as.numeric(Month - min(Month) + 1))
```

```{r}
#| label: pbsdata
#| code-fold: false
pbs
```

The data set is a `tsibble` object, which is a special type of tibble designed for time series data. Two columns have special purposes: the `Month` column is the time index, defining when each observation was made; and the `ATC2` column is a key variable, defining the separate time series in the data set. This data set contains 84 separate time series, one for each `ATC2` value.

Let's first look at the time series for just one ATC group: A12 (Mineral supplements). In fact, this is the same series as was shown in the top panel of @fig-tsparadigms.

```{r}
#| label: fig-a12
#| fig-asp: 0.36
#| fig-cap: The monthly script volume for mineral supplements (ATC group A12) on the Australian PBS.
pbs |>
  filter(ATC2 == "A12") |>
  autoplot(Scripts) +
  labs(title = "Scripts for ATC group A12 (Mineral supplements)")
```

Here there has been a sudden drop in sales at the end of 2005, most likely because of some products no longer being eligible for subsidy. There can also be sudden jumps in sales when a new product becomes available, or a new class of drugs is added to the scheme.

The goal of surveillance is to identify these anomalies as soon as they occur. We can do this by fitting a model to the data, and then comparing the observed values to the model's forecasts. If the observed values are very different from the forecast, then we have an anomaly.

Statistical forecasting models provide forecasts in the form of probability distributions. Let $y_t$ denote the observation of a time series at time $t$. Then a forecast can be expressed as a conditional distribution
$$
  f(y_{t+h} | y_1, \dots, y_{t}, \bm{x}_t),
$$
where $y_1,\dots,y_t$ denotes the observed history of the series, and $\bm{x}_t$ contains any other information available at time $t$ that is used in the model. The forecast "horizon" is given by $h$, denoting the number of time periods into the future that we wish to forecast. Different forecasting methods use different conditioning information, and result in a different form of the forecast distribution. See @fpp3 for a detailed discussion of forecasting methods.

Once we observe the value of $y_{t+h}$, we can calculate the corresponding density scores in the same way as we discussed in @sec-lookout. Because we are interested in real-time surveillance, we need only consider the one-step-ahead forecast density, $f(y_{t+1} | y_1, \dots, y_{t})$. We can then calculate the density score as
$$
  s_{t+1} = -\log \hat{f}(y_{t+1} | y_1, \dots, y_{t}, \bm{x}_t).
$$
This needs to be done iteratively for each time period, updating the model as new data becomes available. Since we need some observations with which to fit a model, we can't begin the process at time $t=1$. Instead, we'll begin at time $t=I$, where $I$ is the smallest number of observations with which we can reasonably estimate the time series model. The value of $I$ will depend on the complexity of the model being used. For simple models with few parameters, we may be able to set $I$ to around 20, but for complex models with many parameters, $I$ may need to be much larger.

We also can't estimate the Generalized Pareto Distribution until we have computed sufficient density scores. Fortunately, we are often working with many time series, not just one, and we can use the density scores from all series when computing the GPD. Suppose we have $m$ series we are monitoring, then at time $t$, we will have computed $m(t-I)$ anomaly scores. Usually we would need at least a few hundred anomaly scores before we could reasonably estimate a GPD from the top 10% of the available scores. Suppose we required at least 200 scores, then we could only compute a GPD once $t \ge J$ where $J = 200/m+I$.

We can summarise the anomaly detection algorithm for surveillance as follows. First, we'll change the notation slightly to allow for more than one series. Let $y_{i,t}$ denote the observation of the $i$th series at time $t$.

For each $t = I,I+1,\dots$, and for all series $i=1,\dots,m$:

* Fit a time series model to the series $y_{i,1},\dots,y_{i,t}$, and estimate the one-step forecast density, $f_{i,t+1}(y \mid y_{i,1},\dots,y_{i,t}, \bm{x}_t)$.
* Compute the anomaly score: $s_{i,t+1} = -\log\hat{f}_{i,t+1}(y_{i,t+1}\mid y_{i,1},\dots,y_{i,t}, \bm{x}_t)$.
* If $t\ge J$, fit a Generalized Pareto Distribution $S$ to the top 10% of anomaly scores $\{s_{i,t}\}$, $i=1,\dots,m$, $t=I+1,I+2,\dots,t$.
* Designate $y_{i,t+1}$ as an anomaly if $P(S > s_{i,t+1}) < 0.05$ under the GPD.

To illustrate, let's apply this to the `pbs` data. Starting with $I=36$ (3 years of data), we will fit ETS models to all available series [Ch 8, @fpp3], and forecast one step ahead in each case. Then we repeat the exercise using 37 observations, then 38 observations, and so on. This is known as a "rolling origin forecast" because the forecast rolls forward by one period each iteration. The process can be illustrated as in @fig-tscvplots.

```{r}
#| label: fig-tscvplots
#| fig-cap: Rolling origin forecasts, with the training sets expanding by one observation at each iteration, and one-step forecasts computed for each training set.
#| echo: false
tscv_plot(.init = 8, .step = 1, h = 1) +
  annotate("text",
    x = 9, y = 0, label = "h = 1",
    color = "#c14b14", family = "Fira Sans"
  )
```

Let's step through the process for the first iteration, with $t=36$. We fit ETS models to each of the time series, with the specific ETS model selected according to the characteristics of the series. See [Ch8, @fpp3] for details of how this is done.

```{r}
#| label: pbsfit
#| code-fold: false
pbs_fit <- pbs |>
  filter(t <= 36) |>
  model(ets = ETS(Scripts))
pbs_fit
```

Forecasts are produced from all fitted models by applying the `forecast()` function to the model table.

```{r}
#| label: pbsfc
#| code-fold: false
pbs_fc <- forecast(pbs_fit, h = 1)
pbs_fc
```

The forecasts are in the `Scripts` column. Note that each of them is a Normal distribution where the mean and variance has been estimated using the ETS model. The mean of the forecast distribution is given as `.mean`.

We can compute the density scores using the `log_likelihood()` function from the `distributional` package, and calculate the lookout probabilities.

```{r}
#| label: pbs_scores
#| code-fold: false
pbs_scores <- pbs_fc |>
  rename(fcast = Scripts) |>
  left_join(pbs |> filter(t == 37), by = c("Month", "ATC2")) |>
  mutate(
    s = -distributional::log_likelihood(fcast, Scripts),
    prob = surprisals(s)
  )
pbs_scores
```

This process is repeated as we increase the size of the training set. The following code block carries out the calculations in a loop. Before starting the calculations, we will set up some new columns in which to store the forecasts, scores and probabilities at each iteration.

```{r}
#| label: pbsloop
#| code-fold: false
#| warning: false
#| message: false
compute_pbs_scores <- function(pbs) {
  pbs <- pbs |>
    mutate(s = NA_real_, prob = NA_real_, fmean = NA_real_, fvar = NA_real_)

  for (i in 36:(max(pbs$t) - 1)) {
    # Fit ETS model to all series and compute 1-step forecasts
    pbs_fc <- pbs |>
      filter(t <= i) |>
      model(ets = ETS(Scripts)) |>
      forecast(h = 1) |>
      rename(fcast = Scripts) |>
      filter(!is.na(.mean))
    # Calculate anomaly scores and probabilities
    pbs_scores <- pbs |>
      filter(t == i + 1) |>
      right_join(pbs_fc, by = c("Month", "ATC2")) |>
      mutate(
        newmean = mean(fcast),
        newvar = distributional::variance(fcast),
        news = -distributional::log_likelihood(fcast, Scripts),
        newprob = surprisals(news)
      ) |>
      select(Month, ATC2, news, newprob, newmean, newvar)
    # Add scores to pbs data set
    pbs <- pbs |>
      left_join(pbs_scores, by = c("Month", "ATC2")) |>
      mutate(
        fmean = if_else(!is.na(newmean), newmean, fmean),
        fvar = if_else(!is.na(newvar), newvar, fvar),
        s = if_else(!is.na(news), news, s),
        prob = if_else(!is.na(newprob), newprob, prob)
      ) |>
      select(-news, -newprob, -newmean, -newvar)
  }
  return(pbs)
}
pbs_scores <- compute_pbs_scores(pbs) |> cache("pbs_scores")
```

```{r}
pbs_scores
```

We compare the distribution against the observation for February 2006.

```{r}
#| label: a12plot0
#| eval: false
observed <- pbs |> filter(ATC2 == "A12", Month == yearmonth("2006 Feb"))
fc_a12 |>
  autoplot(a12) +
  geom_point(data = observed, aes(x = Month, y = Scripts)) +
  labs(
    y = "Scripts (thousands)",
    title = "Forecast of A12 scripts: Feb 2006"
  ) +
  theme(legend.position = "none") +
  ylim(35, 145)
```


```{r}
#| label: pbs_scores3
#| eval: false
pbs_anomalies <- pbs |> filter(prob < 0.05)
pbs_anomalies
```



```{r}
#| label: pbs_scores4
#| eval: false
pbs_plot(pbs, pbs_anomalies, "L03")
```

```{r}
#| label: pbs_scores5
#| eval: false
pbs_plot(pbs, pbs_anomalies, "N07")
```

Consecutive anomalies are hard to identify because the preceding anomalies corrupt the model.

```{r}
#| label: pbs_scores6
#| eval: false
pbs_plot(pbs, pbs_anomalies, "R06")
```

A sequence of near anomalies makes it hard to spot a true anomaly.


## Somewhere

* AO vs IO
* tsoutliers package and tsoutliers() function
* smooth() function
*
